######################################################### 
# Akamai WAF Config                                     #
# Version: 1.1                                          #
# Logstash Version: 1.4.2                               #
# Author: negeric                                       #
# URL: https://github.com/negeric/logstash/             #
# Date: October 21, 2014                                #
# Dashboard Name: AkamaiWAF.json                        #
#########################################################     
# @todo                                                 #
#-------------------------------------------------------#
# Change the 'pattern' to 'match' target                # 
#-------------------------------------------------------#            
# Notes                                                 #
#-------------------------------------------------------#           
# Change the GeoIP Database location.                   #
#########################################################
input {
    file {
		path => ["/storage/logs/holding/akamai/waf/*.log"]
        type => "Akamai"
        tags => [ "WAF" ]
		# Try using a custom since_db path.  This way I can wipe
		# out the "last location" markers and restart parsing of
		# of a file
		sincedb_path => "/storage/sincedb/waf/"
    }
}
filter {
	# Drop out log headers and lines that start with a '#'
	if ([message] =~ /^#/) {
		drop{}
	}
	grok {
		patterns_dir => "/opt/logstash/patterns/"
		# Need to update this with a match field
		pattern => '%{AKAMAIDATE:log_date}%{SPACE}%{TIME:log_time}%{SPACE}%{IP:client}%{SPACE}%{WORD:method}%{SPACE}/%{DATA:site}/%{DATA:uri}%{SPACE}%{NUMBER:status}%{SPACE}%{NUMBER:bytes}%{SPACE}%{NUMBER:time_taken}%{SPACE}"%{DATA:referer}"%{SPACE}"%{DATA:user_agent}"%{SPACE}"%{DATA:cookie}"%{SPACE}\"(?:%{WORD:waf_policy}\|%{DATA:waf_alert_rules}\|%{DATA:waf_deny_rules}|-)\"'
	}
	#Convert values to proper value type
	mutate {
		# Used this for a custom dashboard
		convert => [ "bytes", "integer" ]
	}
	mutate {
		# Combining log_date and log_time.  This will be used as our @timestamp value later
		add_field => { "dt" => "%{log_date} %{log_time}" }
	}

	# Replace IPs with Hostnames
	# If your akamai config uses an IP as the origin server
	# The logs will show hits from /1.2.3.4/uri/object.html
	# Here you can replace the IP with a hostname
	if [site] == "1.2.3.4" {
		mutate {
			replace => [ "Site", "site.dom.com" ]
		}
	}

	#GeoIP
	geoip {
		#Change me to reflect your GeoIP database location
		database => "/opt/logstash/vendor/geoip/GeoLiteCity.dat"
		source => "client"
		target => "GeoIP"
	}

	#Add timestamp based on log entry date
	date {
		match => [ "dt", 'Y-M-d HH:mm:ss' ]
		target => "@timestamp"
	}
}

output {
	if [type] == "Akamai" {
		elasticsearch { host => localhost }
		#stdout { codec => json }
	}
}
